{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Bag of Words Meets Bags of Popcorn - II\n",
    "###### This is a tutorial competiton from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial), the goal is to build a binary classification model to predict the sentiment of a movie review.\n",
    "\n",
    "The following code contains two models: logistic regression and ensemble model of logistic+SVM.  \n",
    "For part-1, logistic model, leaderboard score is 0.95498 (Probability), 0.88604 (Binary).  \n",
    "For part-2, ensemble model, leaderboard score is 0.95522 (Probability), 0.88792 (Binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "When try to remove something from the text other than HTML tags, it may play an important role in the accuracy. Single punctuation may have few effect, how about ':-D' or ':-<'? (If contains [emoji code](https://www.emoji.codes/), it definitely affect more than a single world. Anyway, these datasets seem not to have emoji code. I haven't checked about that. Just another story...) How about numbers? Here I retain numbers in text after compared with the accuracyof training from non-number text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('E:/542/Pro4/labeledTrainData.tsv', \n",
    "                  header=0,delimiter='\\t',quoting=3)\n",
    "test = pd.read_csv(\"E:/542/Pro4/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess (corpus):\n",
    "    num=corpus['review'].size\n",
    "    clean=[]\n",
    "    for i in range(0, num):\n",
    "        review_text = BeautifulSoup(corpus[\"review\"][i],'html.parser').get_text()\n",
    "        pattern = re.compile(r'[^\\w\\s]')\n",
    "        letnum_only = re.sub(pattern, \" \", review_text) \n",
    "        clean.append(letnum_only)\n",
    "    clean_result=pd.DataFrame({'text':clean}) \n",
    "    return clean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmer = EnglishStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_df=preprocess(train)\n",
    "clean_test_df=preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(clean_train_df['text'], train['sentiment'],  test_size=0.25, random_state=check_random_state(888))\n",
    "X_train2, X_test2, y_train2, y_test2= train_test_split(clean_train_df['text'], train['sentiment'],  test_size=0.25, random_state=check_random_state(1000))\n",
    "X_train3, X_test3, y_train3, y_test3= train_test_split(clean_train_df['text'], train['sentiment'],  test_size=0.25)\n",
    "X_trainf, X_testf, y_trainf, y_testf= train_test_split(clean_train_df['text'], train['sentiment'],  test_size=0.25, random_state=check_random_state(512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble method\n",
    "### Single model\n",
    "Before use ensemble method, check the performance of each single model on validation dataset first. It seems Naive Bayes has a lower accuracy than other two. Although this rank maybe different by tuning parameters, I decide to leave it aside.\n",
    "\n",
    "Another issue you may find is that [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) only returns probability when loss function is 'log' or 'modified_huber'. Here use 'hinge' which gives a linear SVM. Then when try to combine two model and generate probabilistic predictions, I just use binary results of SGDClassifiers. This may affect the variance of its predictions, but if regard it as a function, it become acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model1(X_train, y_train, X_test, random_state):\n",
    "    tfidf=TfidfVectorizer(stop_words = 'english',min_df=5,max_df=0.15,ngram_range=(1, 4),tokenizer=tokenize)\n",
    "    train_X = tfidf.fit_transform(X_train)\n",
    "    test_X=tfidf.transform(X_test)\n",
    "    \n",
    "    model=LogisticRegression(penalty='l2', dual=True,random_state=random_state)\n",
    "    model.fit(train_X,y_train)\n",
    "    y_pred=model.predict(test_X)\n",
    "    y_score=model.predict_proba(test_X)    \n",
    "    return y_pred,y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model2(X_train, y_train, X_test, random_state):\n",
    "    tfidf=TfidfVectorizer(stop_words = 'english',min_df=5,max_df=0.15,ngram_range=(1, 4),tokenizer=tokenize)\n",
    "    train_X = tfidf.fit_transform(X_train)\n",
    "    test_X=tfidf.transform(X_test)\n",
    "    \n",
    "    model2=BernoulliNB(alpha=0.15)\n",
    "    model2.fit(train_X,y_train)\n",
    "    y_pred2=model2.predict(test_X)\n",
    "    y_score2=model2.predict_proba(test_X)\n",
    "    return y_pred2,y_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model3(X_train, y_train, X_test, random_state):\n",
    "    tfidf=TfidfVectorizer(stop_words = 'english',min_df=5,max_df=0.15,ngram_range=(1, 4),tokenizer=tokenize)\n",
    "    train_X = tfidf.fit_transform(X_train)\n",
    "    test_X=tfidf.transform(X_test)\n",
    "    model3=SGDClassifier(random_state=random_state)\n",
    "    #model3=svm.LinearSVC(random_state=random_state)\n",
    "    model3.fit(train_X,y_train)\n",
    "    y_pred3=model3.predict(test_X)\n",
    "    return y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction accuracy = 89.3%\n",
      "Naive Bayes prediction accuracy = 87.8%\n",
      "SGDClassifier(SVM) prediction accuracy = 89.3%\n"
     ]
    }
   ],
   "source": [
    "y_pred1, y_score1= model1(X_train, y_train, X_test,random_state=check_random_state(800))\n",
    "y_pred2,y_score2= model2(X_train2, y_train2, X_test2,random_state=check_random_state(255))\n",
    "y_pred3= model3(X_train3, y_train3, X_test3,random_state=check_random_state(1024))\n",
    "score1 = accuracy_score(y_pred1, y_test)\n",
    "score2 = accuracy_score(y_pred2, y_test2)\n",
    "score3 = accuracy_score(y_pred3, y_test3)\n",
    "print(\"Logistic Regression prediction accuracy = {0:3.1f}%\".format(100.0 * score1))\n",
    "print(\"Naive Bayes prediction accuracy = {0:3.1f}%\".format(100.0 * score2))\n",
    "print(\"SGDClassifier(SVM) prediction accuracy = {0:3.1f}%\".format(100.0 * score3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred1f,y_score1f= model1(X_train, y_train, X_testf,random_state=check_random_state(800))\n",
    "#y_pred2f,y_score2f= model2(X_train2, y_train2, X_testf,random_state=check_random_state(255))\n",
    "y_pred3f= model3(X_train3, y_train3, X_testf,random_state=check_random_state(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finallist=[0 for i in range(0,6250)]\n",
    "finaldf=pd.DataFrame({'Sentiment':finallist})\n",
    "for i in range(0,6250):\n",
    "    ss=y_score1f[i][1]+y_pred3f[i]\n",
    "    if ss>=1.5:\n",
    "        finaldf['Sentiment'][i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94368\n"
     ]
    }
   ],
   "source": [
    "scorefinal = accuracy_score(finaldf['Sentiment'], y_testf)\n",
    "print(scorefinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Prediction of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred1F,y_score1F= model1(clean_train_df['text'], train['sentiment'], clean_test_df['text'],random_state=check_random_state(800))\n",
    "#y_pred2F,y_score2F= model2(clean_train_df['text'], train['sentiment'], clean_test_df['text'],random_state=check_random_state(255))\n",
    "y_pred3F= model3(clean_train_df['text'], train['sentiment'], clean_test_df['text'],random_state=check_random_state(1024))\n",
    "Finallist=[0 for i in range(0,25000)]\n",
    "Finallist2=[0 for i in range(0,25000)]\n",
    "for i in range(0,25000):\n",
    "    thres=y_score1F[i][1]+y_pred3F[i]\n",
    "    thres2=y_pred1F[i]+y_pred3F[i]\n",
    "    Finallist[i]=thres/2\n",
    "    if thres2>=1.5:\n",
    "        Finallist2[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame({'id':test['id'],'sentiment':Finallist})\n",
    "output.to_csv(\"PredEnsemProb.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output2=pd.DataFrame({'id':test['id'],'sentiment':Finallist2})\n",
    "output2.to_csv(\"PredEnsemBin.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
